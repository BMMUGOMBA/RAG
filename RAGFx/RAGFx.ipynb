{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f271668-1b2e-4537-9ef7-ceef62b7d78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Environment Setup - Complete Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff19db5c-0181-473b-ba2d-685225ecb1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "271efdf2-75dc-4a24-99e5-47f0182f7be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]\n",
      "Platform: Windows-11-10.0.26100-SP0\n",
      "Architecture: ('64bit', 'WindowsPE')\n",
      "⚠️  Consider using a virtual environment for this project\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Architecture: {platform.architecture()}\")\n",
    "\n",
    "# Check if we're in a virtual environment (recommended)\n",
    "if hasattr(sys, 'real_prefix') or (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix):\n",
    "    print(\"✓ Running in virtual environment\")\n",
    "else:\n",
    "    print(\"⚠️  Consider using a virtual environment for this project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31600b1b-372b-4b06-b169-68c937e4ede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Core Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7dc9fe9-1204-4c61-83cd-25a04830af3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages for RAG system...\n",
      "✓ Successfully installed sentence-transformers\n",
      "✓ Successfully installed faiss-cpu\n",
      "✓ Successfully installed chromadb\n",
      "✓ Successfully installed pandas\n",
      "✓ Successfully installed numpy\n",
      "✓ Successfully installed scikit-learn\n",
      "✓ Successfully installed nltk\n",
      "✓ Successfully installed tqdm\n",
      "✓ Successfully installed matplotlib\n",
      "✓ Successfully installed seaborn\n",
      "✓ Successfully installed jupyter\n",
      "✓ Successfully installed ipywidgets\n"
     ]
    }
   ],
   "source": [
    "# Install all required packages for the RAG system\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"✓ Successfully installed {package}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"✗ Failed to install {package}: {e}\")\n",
    "\n",
    "# Core packages for RAG system\n",
    "packages = [\n",
    "    \"sentence-transformers\",  # For local embeddings\n",
    "    \"faiss-cpu\",             # Vector database (CPU version)\n",
    "    \"chromadb\",              # Alternative vector database\n",
    "    \"pandas\",                # Data manipulation\n",
    "    \"numpy\",                 # Numerical operations\n",
    "    \"scikit-learn\",          # ML utilities\n",
    "    \"nltk\",                  # Natural language processing\n",
    "    \"tqdm\",                  # Progress bars\n",
    "    \"matplotlib\",            # Plotting\n",
    "    \"seaborn\",               # Better plotting\n",
    "    \"jupyter\",               # Jupyter notebook support\n",
    "    \"ipywidgets\",            # Interactive widgets\n",
    "]\n",
    "\n",
    "print(\"Installing packages for RAG system...\")\n",
    "for package in packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcdf54ac-874c-45cc-a3a0-db1363f3662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Optional Packages (GPU Support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a454887-f225-4fdd-9dea-d4a94d8038f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️  CUDA not available, using CPU versions\n"
     ]
    }
   ],
   "source": [
    "# Optional: Install GPU-accelerated packages if you have CUDA\n",
    "import torch\n",
    "\n",
    "def check_gpu_support():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"✓ CUDA available: {torch.cuda.get_device_name()}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"ℹ️  CUDA not available, using CPU versions\")\n",
    "        return False\n",
    "\n",
    "has_gpu = check_gpu_support()\n",
    "\n",
    "# Install GPU versions if available\n",
    "if has_gpu:\n",
    "    gpu_packages = [\n",
    "        \"faiss-gpu\",  # GPU version of FAISS (will replace faiss-cpu)\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nInstalling GPU-accelerated packages...\")\n",
    "    for package in gpu_packages:\n",
    "        install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99ed93ff-1c87-47f0-81fb-b5ba35e7963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download and Setup NLTK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33253337-f20b-43e2-a945-4456886ca019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK data...\n",
      "✓ Downloaded punkt\n",
      "✓ Downloaded stopwords\n",
      "✓ Downloaded wordnet\n",
      "✓ Downloaded averaged_perceptron_tagger\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data for text processing\n",
    "import nltk\n",
    "\n",
    "nltk_downloads = [\n",
    "    'punkt',        # Sentence tokenizer\n",
    "    'stopwords',    # Stop words\n",
    "    'wordnet',      # WordNet lemmatizer\n",
    "    'averaged_perceptron_tagger',  # POS tagger\n",
    "]\n",
    "\n",
    "print(\"Downloading NLTK data...\")\n",
    "for item in nltk_downloads:\n",
    "    try:\n",
    "        nltk.download(item, quiet=True)\n",
    "        print(f\"✓ Downloaded {item}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to download {item}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00b5ef73-8ac2-4646-a36e-70178b214a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and Test All Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4bfce95-f03a-4950-89c1-9e11429f5c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Core libraries imported\n",
      "✓ ML/NLP libraries imported\n",
      "✓ Utility libraries imported\n"
     ]
    }
   ],
   "source": [
    "# Import all libraries and test they work correctly\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output\n",
    "\n",
    "# Core imports\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import os\n",
    "    import re\n",
    "    from pathlib import Path\n",
    "    from typing import List, Dict, Tuple, Optional\n",
    "    print(\"✓ Core libraries imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Core import error: {e}\")\n",
    "\n",
    "# ML and NLP imports\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import faiss\n",
    "    import chromadb\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import nltk\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    print(\"✓ ML/NLP libraries imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ ML/NLP import error: {e}\")\n",
    "\n",
    "# Utility imports\n",
    "try:\n",
    "    from tqdm.notebook import tqdm  # Progress bars for Jupyter\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    print(\"✓ Utility libraries imported\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Utility import error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fe2db04-331d-4342-b194-101c3f998b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#System Resource Check"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0471128e-3864-4596-a2ac-c85ca1ce6d04",
   "metadata": {},
   "source": [
    "# Check system resources for optimal configuration\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "def check_system_resources():\n",
    "    # Memory info\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Total RAM: {memory.total / (1024**3):.2f} GB\")\n",
    "    print(f\"Available RAM: {memory.available / (1024**3):.2f} GB\")\n",
    "    print(f\"RAM Usage: {memory.percent}%\")\n",
    "    \n",
    "    # CPU info\n",
    "    print(f\"CPU Cores: {psutil.cpu_count(logical=False)} physical, {psutil.cpu_count(logical=True)} logical\")\n",
    "    \n",
    "    # Disk space check\n",
    "    disk = psutil.disk_usage('.')\n",
    "    print(f\"Disk Space: {disk.free / (1024**3):.2f} GB free of {disk.total / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # GPU info\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_count = torch.cuda.device_count()\n",
    "            print(f\"GPUs available: {gpu_count}\")\n",
    "            for i in range(gpu_count):\n",
    "                gpu_props = torch.cuda.get_device_properties(i)\n",
    "                gpu_memory = gpu_props.total_memory / (1024**3)\n",
    "                print(f\"  GPU {i}: {gpu_props.name} ({gpu_memory:.2f} GB)\")\n",
    "        else:\n",
    "            print(\"No GPU available\")\n",
    "    except:\n",
    "        print(\"PyTorch not available for GPU check\")\n",
    "\n",
    "check_system_resources()\n",
    "\n",
    "# Recommend configuration based on resources\n",
    "def recommend_config():\n",
    "    memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "    \n",
    "    print(\"\\n=== Recommended Configuration ===\")\n",
    "    if memory_gb >= 32:\n",
    "        print(\"High-end setup: Use large embedding models and ChromaDB\")\n",
    "        return {\"embedding_model\": \"all-mpnet-base-v2\", \"vector_db\": \"chromadb\", \"batch_size\": 100}\n",
    "    elif memory_gb >= 16:\n",
    "        print(\"Mid-range setup: Use medium embedding models and FAISS\")\n",
    "        return {\"embedding_model\": \"all-MiniLM-L6-v2\", \"vector_db\": \"faiss\", \"batch_size\": 50}\n",
    "    elif memory_gb >= 8:\n",
    "        print(\"Budget setup: Use lightweight models and small batches\")\n",
    "        return {\"embedding_model\": \"all-MiniLM-L6-v2\", \"vector_db\": \"faiss\", \"batch_size\": 25}\n",
    "    else:\n",
    "        print(\"Low memory: Use very lightweight setup\")\n",
    "        return {\"embedding_model\": \"paraphrase-MiniLM-L3-v2\", \"vector_db\": \"faiss\", \"batch_size\": 10}\n",
    "\n",
    "config = recommend_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfb22175-4ad5-4f34-84c2-779dd44325d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 15.86 GB\n",
      "Available RAM: 3.98 GB\n",
      "RAM Usage: 74.9%\n",
      "CPU Cores: 4 physical, 8 logical\n",
      "Disk Space: 13.02 GB free of 236.95 GB (94.5% used)\n",
      "No GPU available\n",
      "\n",
      "=== Recommended Configuration ===\n",
      "Budget setup: Use lightweight models and small batches\n"
     ]
    }
   ],
   "source": [
    "# Check system resources for optimal configuration\n",
    "import psutil\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "def _pretty_gb(x): \n",
    "    return f\"{x/(1024**3):.2f} GB\"\n",
    "\n",
    "def safe_disk_usage(path=\".\"):\n",
    "    \"\"\"Robust disk usage that avoids psutil edge cases on Windows.\"\"\"\n",
    "    try:\n",
    "        # Try psutil with the resolved path first\n",
    "        return psutil.disk_usage(str(Path(path).resolve()))\n",
    "    except Exception:\n",
    "        # Fall back to drive root (Windows) or \"/\" (POSIX)\n",
    "        anchor = Path(path).resolve().anchor or (Path(path).anchor or \"/\")\n",
    "        try:\n",
    "            return psutil.disk_usage(anchor)\n",
    "        except Exception:\n",
    "            # Final fallback: shutil (cross-platform)\n",
    "            total, used, free = shutil.disk_usage(anchor or \"/\")\n",
    "            # Mimic psutil result\n",
    "            from collections import namedtuple\n",
    "            DiskUsage = namedtuple(\"sdiskusage\", [\"total\", \"used\", \"free\", \"percent\"])\n",
    "            percent = (used / total * 100) if total else 0.0\n",
    "            return DiskUsage(total, used, free, percent)\n",
    "\n",
    "def check_system_resources():\n",
    "    # Memory info\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Total RAM: {_pretty_gb(memory.total)}\")\n",
    "    print(f\"Available RAM: {_pretty_gb(memory.available)}\")\n",
    "    print(f\"RAM Usage: {memory.percent}%\")\n",
    "    \n",
    "    # CPU info\n",
    "    print(f\"CPU Cores: {psutil.cpu_count(logical=False)} physical, {psutil.cpu_count(logical=True)} logical\")\n",
    "    \n",
    "    # Disk space (robust)\n",
    "    disk = safe_disk_usage(\".\")\n",
    "    print(f\"Disk Space: {_pretty_gb(disk.free)} free of {_pretty_gb(disk.total)} ({disk.percent:.1f}% used)\")\n",
    "    \n",
    "    # GPU info\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_count = torch.cuda.device_count()\n",
    "            print(f\"GPUs available: {gpu_count}\")\n",
    "            for i in range(gpu_count):\n",
    "                props = torch.cuda.get_device_properties(i)\n",
    "                print(f\"  GPU {i}: {props.name} ({_pretty_gb(props.total_memory)})\")\n",
    "        else:\n",
    "            print(\"No GPU available\")\n",
    "    except Exception:\n",
    "        print(\"PyTorch not available for GPU check\")\n",
    "\n",
    "check_system_resources()\n",
    "\n",
    "# Recommend configuration based on resources\n",
    "def recommend_config():\n",
    "    memory_gb = psutil.virtual_memory().total / (1024**3)\n",
    "    print(\"\\n=== Recommended Configuration ===\")\n",
    "    if memory_gb >= 32:\n",
    "        print(\"High-end setup: Use large embedding models and ChromaDB\")\n",
    "        return {\"embedding_model\": \"all-mpnet-base-v2\", \"vector_db\": \"chromadb\", \"batch_size\": 100}\n",
    "    elif memory_gb >= 16:\n",
    "        print(\"Mid-range setup: Use medium embedding models and FAISS\")\n",
    "        return {\"embedding_model\": \"all-MiniLM-L6-v2\", \"vector_db\": \"faiss\", \"batch_size\": 50}\n",
    "    elif memory_gb >= 8:\n",
    "        print(\"Budget setup: Use lightweight models and small batches\")\n",
    "        return {\"embedding_model\": \"all-MiniLM-L6-v2\", \"vector_db\": \"faiss\", \"batch_size\": 25}\n",
    "    else:\n",
    "        print(\"Low memory: Use very lightweight setup\")\n",
    "        return {\"embedding_model\": \"paraphrase-MiniLM-L3-v2\", \"vector_db\": \"faiss\", \"batch_size\": 10}\n",
    "\n",
    "config = recommend_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7282e776-8d20-4484-a541-d86ee73b9cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Project Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0601d7cb-22e7-429a-99a1-41b9c1d485a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created directory: data/raw\n",
      "✓ Created directory: data/processed\n",
      "✓ Created directory: data/embeddings\n",
      "✓ Created directory: models\n",
      "✓ Created directory: vector_db\n",
      "✓ Created directory: outputs\n",
      "✓ Created directory: notebooks\n",
      "✓ Created directory: utils\n",
      "✓ Created .gitignore file\n"
     ]
    }
   ],
   "source": [
    "# Create organized directory structure for your RAG project\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def create_project_structure():\n",
    "    directories = [\n",
    "        \"data/raw\",           # Your original text files and JSON\n",
    "        \"data/processed\",     # Cleaned and chunked data\n",
    "        \"data/embeddings\",    # Saved embeddings\n",
    "        \"models\",            # Downloaded models\n",
    "        \"vector_db\",         # Vector database files\n",
    "        \"outputs\",           # Query results and logs\n",
    "        \"notebooks\",         # Additional notebooks\n",
    "        \"utils\",             # Helper functions\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"✓ Created directory: {directory}\")\n",
    "    \n",
    "    # Create .gitignore for the project\n",
    "    gitignore_content = \"\"\"\n",
    "# Models and embeddings (too large for git)\n",
    "models/\n",
    "data/embeddings/\n",
    "vector_db/\n",
    "\n",
    "# Jupyter notebook checkpoints\n",
    ".ipynb_checkpoints/\n",
    "\n",
    "# Python cache\n",
    "__pycache__/\n",
    "*.pyc\n",
    "\n",
    "# Environment variables\n",
    ".env\n",
    "\n",
    "# Large data files\n",
    "*.bin\n",
    "*.gguf\n",
    "*.model\n",
    "\"\"\"\n",
    "    \n",
    "    with open('.gitignore', 'w') as f:\n",
    "        f.write(gitignore_content)\n",
    "    print(\"✓ Created .gitignore file\")\n",
    "\n",
    "create_project_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff5a85e-504d-4377-9e43-0fe0e3d208e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
